<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>ODIN: An OmniDirectional INdoor Dataset Capturing Activities of Daily Living From Multiple Synchronized Modalities | Théo Morales</title>
<meta name="keywords" content="Pose estimation, Pipelines, Cameras, Physiology, Recording, Pattern recognition">
<meta name="description" content="We present ODIN, a large-scale multi-modal dataset for human behavior understanding using top-view omnidirectional cameras.">
<meta name="author" content="Siddharth Ravi,&amp;thinsp;Pau Climent-Perez,&amp;thinsp;Théo Morales,&amp;thinsp;Carlo Huesca-Spairani,&amp;thinsp;Kooshan Hashemifard,&amp;thinsp;Francisco Flórez-Revuelta">
<link rel="canonical" href="https://www.theomorales.com/ODIN/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.f862e423d584dcad80ec00d158460a9f311e7d075e2f9c146af7e938d6cf0d7b.css" integrity="sha256-&#43;GLkI9WE3K2A7ADRWEYKnzEefQdeL5wUavfpONbPDXs=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://www.theomorales.com/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://www.theomorales.com/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://www.theomorales.com/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://www.theomorales.com/apple-touch-icon.png">

<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://www.theomorales.com/ODIN/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript><meta property="og:title" content="ODIN: An OmniDirectional INdoor Dataset Capturing Activities of Daily Living From Multiple Synchronized Modalities" />
<meta property="og:description" content="We present ODIN, a large-scale multi-modal dataset for human behavior understanding using top-view omnidirectional cameras." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://www.theomorales.com/ODIN/" /><meta property="article:section" content="publications" />
<meta property="article:published_time" content="2023-06-10T00:00:00+00:00" />
<meta property="article:modified_time" content="2023-06-10T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="ODIN: An OmniDirectional INdoor Dataset Capturing Activities of Daily Living From Multiple Synchronized Modalities"/>
<meta name="twitter:description" content="We present ODIN, a large-scale multi-modal dataset for human behavior understanding using top-view omnidirectional cameras."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Publications",
      "item": "https://www.theomorales.com/publications/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "ODIN: An OmniDirectional INdoor Dataset Capturing Activities of Daily Living From Multiple Synchronized Modalities",
      "item": "https://www.theomorales.com/ODIN/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "ODIN: An OmniDirectional INdoor Dataset Capturing Activities of Daily Living From Multiple Synchronized Modalities",
  "name": "ODIN: An OmniDirectional INdoor Dataset Capturing Activities of Daily Living From Multiple Synchronized Modalities",
  "description": "We present ODIN, a large-scale multi-modal dataset for human behavior understanding using top-view omnidirectional cameras.",
  "keywords": [
    "Pose estimation", "Pipelines", "Cameras", "Physiology", "Recording", "Pattern recognition"
  ],
  "articleBody": " Download Paper Dataset Abstract We introduce ODIN (the OmniDirectional INdoor dataset), the first large-scale multi-modal dataset aimed at spurring research using top-view omnidirectional cameras in challenges related to human behaviour understanding. Recorded in real-life indoor environments with varying levels of occlusion, the dataset contains images of participants performing various activities of daily living. Along with omnidirectional images, additional synchronized modalities of data are provided. These include (1) RGB, infrared, and depth images from multiple RGB-D cameras, (2) egocentric videos, (3) physiological signals and accelerometer readings from a smart bracelet, and (4) 3D scans of the recording environments. To the best of our knowledge, ODIN is also the first dataset to provide camera-frame 3D human pose estimates for omnidirectional images, which are obtained using our novel pipeline. The project is open sourced and available at https://odin-dataset.github.io .\nFigure 3: Overview of ODIN, a large-scale omnidirectional dataset for Human Behaviour Understanding – Each sequence is composed of the 3D scan of the recording location and omnidirectional RGB images as well as 5 extra modalities: (1) depth, (2) IR, (3) RGB images from side views, (4) RGB egocentric images and (5) biometric signals from a wearable device. Four of the five environments — kitchen, living room, bathroom, and bedroom — are represented in the figure (the activity room can be seen in Fig. 4). Citation Ravi, Siddharth et al. “ODIN: An OmniDirectional INdoor Dataset Capturing Activities of Daily Living From Multiple Synchronized Modalities” 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW) (2023): 1-8.\n@article{Ravi2023ODINAO, title={ODIN: An OmniDirectional INdoor dataset capturing Activities of Daily Living from multiple synchronized modalities}, author={Siddharth Ravi and Pau Climent-P{\\'e}rez and Th{\\'e}o Morales and Carlo Huesca-Spairani and Kooshan Hashemifard and Francisco Fl{\\'o}rez-Revuelta}, journal={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)}, year={2023}, pages={6488-6497}, url={https://api.semanticscholar.org/CorpusID:260913045} } Related material Project website ",
  "wordCount" : "304",
  "inLanguage": "en",
  "datePublished": "2023-06-10T00:00:00Z",
  "dateModified": "2023-06-10T00:00:00Z",
  "author":[{
    "@type": "Person",
    "name": "Siddharth Ravi"
  }, {
    "@type": "Person",
    "name": "Pau Climent-Perez"
  }, {
    "@type": "Person",
    "name": "Théo Morales"
  }, {
    "@type": "Person",
    "name": "Carlo Huesca-Spairani"
  }, {
    "@type": "Person",
    "name": "Kooshan Hashemifard"
  }, {
    "@type": "Person",
    "name": "Francisco Flórez-Revuelta"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://www.theomorales.com/ODIN/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Théo Morales",
    "logo": {
      "@type": "ImageObject",
      "url": "https://www.theomorales.com/favicon.ico"
    }
  }
}
</script>



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.css" integrity="sha384-RZU/ijkSsFbcmivfdRBQDtwuwVqK7GMOw6IMvKyeWL2K5UAlyp6WonmB8m7Jd0Hn" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.js" integrity="sha384-pK1WpvzWVBQiP0/GjnvRxV4mOb0oxFuyRxJlk6vVw146n3egcN5C925NCP7a7BY8" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/contrib/auto-render.min.js" integrity="sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl" crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          
          
          delimiters: [
            {left: '$$', right: '$$', display: true},
            {left: '$', right: '$', display: false},
            {left: "\\begin{equation}", right: "\\end{equation}", display: true},
            {left: "\\begin{equation*}", right: "\\end{equation*}", display: true},
            {left: "\\begin{align}", right: "\\end{align}", display: true},
            {left: "\\begin{align*}", right: "\\end{align*}", display: true},
            {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
            {left: "\\begin{gather}", right: "\\end{gather}", display: true},
            {left: "\\begin{CD}", right: "\\end{CD}", display: true},
          ],
          
          throwOnError : false
        });
    });
</script>
 


</head>

<body class="" id="top">

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://www.theomorales.com/" accesskey="h" title="Théo Morales">
             
                <img src="https://www.theomorales.com/favicon.png" alt="" aria-label="logo"
                    height="18"
                    width="18">Théo Morales</a>
            <div class="logo-switches">
                <ul class="lang-switch"><li>|</li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://www.theomorales.com/publications/" title="Publications">
                    <span>Publications</span>
                </a>
            </li>
            <li>
                <a href="https://www.theomorales.com/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://notes.theomorales.com" title="Blog">
                    <span>Blog</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
        </ul>
    </nav>
</header>

    <main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      ODIN: An OmniDirectional INdoor Dataset Capturing Activities of Daily Living From Multiple Synchronized Modalities
    </h1>

    
    <div class="post-meta">&lt;span title=&#39;2023-06-10 00:00:00 &#43;0000 UTC&#39;&gt;June 2023&lt;/span&gt;&amp;nbsp;&amp;middot;&amp;nbsp;Siddharth Ravi,&amp;amp;thinsp;Pau Climent-Perez,&amp;amp;thinsp;Théo Morales,&amp;amp;thinsp;Carlo Huesca-Spairani,&amp;amp;thinsp;Kooshan Hashemifard,&amp;amp;thinsp;Francisco Flórez-Revuelta&nbsp;&middot;&nbsp;<a href="https://doi.org/10.1109/CVPRW59228.2023.00690" rel="noopener noreferrer" target="_blank">2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</a>

</div>
  </header> 
  <div class="post-content"><hr>
<h5 id="download">Download</h5>
<ul>
<li><a href="/3.pdf">Paper</a>
</li>
<li><a href="https://odin-dataset.github.io" target="_blank">Dataset</a>
</li>
</ul>
<!--+ [Poster](/2p.pdf)-->
<hr>
<h5 id="abstract">Abstract</h5>
<p>We introduce ODIN (the OmniDirectional INdoor dataset), the first large-scale multi-modal dataset aimed at spurring research using top-view omnidirectional cameras in challenges related to human behaviour understanding. Recorded in real-life indoor environments with varying levels of occlusion, the dataset contains images of participants performing various activities of daily living. Along with omnidirectional images, additional synchronized modalities of data are provided. These include (1) RGB, infrared, and depth images from multiple RGB-D cameras, (2) egocentric videos, (3) physiological signals and accelerometer readings from a smart bracelet, and (4) 3D scans of the recording environments. To the best of our knowledge, ODIN is also the first dataset to provide camera-frame 3D human pose estimates for omnidirectional images, which are obtained using our novel pipeline. The project is open sourced and available at <a href="https://odin-dataset.github.io" target="_blank">https://odin-dataset.github.io</a>
.</p>
<hr>
<h5 id="figure-3-overview-of-odin-a-large-scale-omnidirectional-dataset-for-human-behaviour-understanding--each-sequence-is-composed-of-the-3d-scan-of-the-recording-location-and-omnidirectional-rgb-images-as-well-as-5-extra-modalities-1-depth-2-ir-3-rgb-images-from-side-views-4-rgb-egocentric-images-and-5-biometric-signals-from-a-wearable-device-four-of-the-five-environments--kitchen-living-room-bathroom-and-bedroom--are-represented-in-the-figure-the-activity-room-can-be-seen-in-fig-4">Figure 3: Overview of ODIN, a large-scale omnidirectional dataset for Human Behaviour Understanding – Each sequence is composed of the 3D scan of the recording location and omnidirectional RGB images as well as 5 extra modalities: (1) depth, (2) IR, (3) RGB images from side views, (4) RGB egocentric images and (5) biometric signals from a wearable device. Four of the five environments — kitchen, living room, bathroom, and bedroom — are represented in the figure (the activity room can be seen in Fig. 4).</h5>
<p><img loading="lazy" src="/3.png" alt=""  />
</p>
<h5 id="citation">Citation</h5>
<p>Ravi, Siddharth et al. “ODIN: An OmniDirectional INdoor Dataset Capturing Activities of Daily Living From Multiple Synchronized Modalities” <em>2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW) (2023)</em>: 1-8.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-BibTeX" data-lang="BibTeX"><span style="display:flex;"><span><span style="color:#0a0;text-decoration:underline">@article</span>{Ravi2023ODINAO,
</span></span><span style="display:flex;"><span>  <span style="color:#1e90ff">title</span>=<span style="color:#a50">{ODIN: An OmniDirectional INdoor dataset capturing Activities of Daily Living from multiple synchronized modalities}</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#1e90ff">author</span>=<span style="color:#a50">{Siddharth Ravi and Pau Climent-P{\&#39;e}rez and Th{\&#39;e}o Morales and Carlo Huesca-Spairani and Kooshan Hashemifard and Francisco Fl{\&#39;o}rez-Revuelta}</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#1e90ff">journal</span>=<span style="color:#a50">{2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)}</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#1e90ff">year</span>=<span style="color:#a50">{2023}</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#1e90ff">pages</span>=<span style="color:#a50">{6488-6497}</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#1e90ff">url</span>=<span style="color:#a50">{https://api.semanticscholar.org/CorpusID:260913045}</span>
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><hr>
<h5 id="related-material">Related material</h5>
<!--+ [Presentation video](https://www.youtube.com/watch?v=YrR-pR9nDT0)-->
<ul>
<li><a href="https://odin-dataset.github.io" target="_blank">Project website</a>
</li>
</ul>

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://www.theomorales.com/tags/pose-estimation/">Pose Estimation</a></li>
      <li><a href="https://www.theomorales.com/tags/pipelines/">Pipelines</a></li>
      <li><a href="https://www.theomorales.com/tags/cameras/">Cameras</a></li>
      <li><a href="https://www.theomorales.com/tags/physiology/">Physiology</a></li>
      <li><a href="https://www.theomorales.com/tags/recording/">Recording</a></li>
      <li><a href="https://www.theomorales.com/tags/pattern-recognition/">Pattern Recognition</a></li>
    </ul>
  </footer>
</article>
    </main>
    

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>
</html>
